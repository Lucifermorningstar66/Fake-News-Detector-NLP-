{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\global village\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import create_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gossip_fake = pd.read_csv('C:\\\\Users\\\\global village\\\\OneDrive\\Desktop\\\\dataset\\\\gossipcop_fake.csv')\n",
    "gossip_real = pd.read_csv('C:\\\\Users\\\\global village\\\\OneDrive\\Desktop\\\\dataset\\\\gossipcop_real.csv')\n",
    "politifact_fake = pd.read_csv('C:\\\\Users\\\\global village\\\\OneDrive\\Desktop\\\\dataset\\\\politifact_fake.csv')\n",
    "politifact_real = pd.read_csv('C:\\\\Users\\\\global village\\\\OneDrive\\Desktop\\\\dataset\\\\politifact_real.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gossip_fake['label'] = 0\n",
    "politifact_fake['label'] = 0\n",
    "gossip_real['label'] = 1\n",
    "politifact_real['label'] = 1\n",
    "\n",
    "df = pd.concat([gossip_fake, gossip_real, politifact_fake, politifact_real])\n",
    "df = df[['title', 'label']]\n",
    "df.dropna(inplace=True)\n",
    "df.rename(columns={'title': 'text'}, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['text'].tolist(), df['label'].tolist(), test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\global village\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\global village\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    "))\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    val_labels\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\global village\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n",
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=len(train_dataset)*3)\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:From C:\\Users\\global village\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tf_keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\global village\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tf_keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "1305/1305 [==============================] - 3136s 2s/step - loss: 0.3980 - accuracy: 0.8286 - val_loss: 0.3465 - val_accuracy: 0.8418\n",
      "Epoch 2/2\n",
      "1305/1305 [==============================] - 2639s 2s/step - loss: 0.2645 - accuracy: 0.8930 - val_loss: 0.3342 - val_accuracy: 0.8664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x15b1a6c1a10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset.shuffle(1000).batch(16),\n",
    "          validation_data=val_dataset.batch(16),\n",
    "          epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./bert_fakenews_model\\\\tokenizer_config.json',\n",
       " './bert_fakenews_model\\\\special_tokens_map.json',\n",
       " './bert_fakenews_model\\\\vocab.txt',\n",
       " './bert_fakenews_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./bert_fakenews_model\")\n",
    "tokenizer.save_pretrained(\"./bert_fakenews_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: Fake\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"Breaking: President announces new climate policy for 2030.\"\n",
    "\n",
    "inputs = tokenizer(sample_text, return_tensors=\"tf\", truncation=True, padding=True, max_length=128)\n",
    "\n",
    "outputs = model(inputs)\n",
    "logits = outputs.logits\n",
    "\n",
    "# (0 = Fake, 1 = Real)\n",
    "predicted_label = tf.argmax(logits, axis=1).numpy()[0]\n",
    "label_name = \"Real\" if predicted_label == 1 else \"Fake\"\n",
    "print(f\"Predicted Label: {label_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fake] Donald Trump buys new football team in Texas.\n",
      "[Real] NASA successfully lands rover on Mars.\n",
      "[Fake] Scientists discover water on the Sun's surface.\n",
      "[Real] Biden declares national emergency over economic crash.\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"Donald Trump buys new football team in Texas.\",\n",
    "    \"NASA successfully lands rover on Mars.\",\n",
    "    \"Scientists discover water on the Sun's surface.\",\n",
    "    \"Biden declares national emergency over economic crash.\"\n",
    "]\n",
    "\n",
    "encodings = tokenizer(texts, return_tensors='tf', padding=True, truncation=True, max_length=128)\n",
    "\n",
    "outputs = model(encodings)\n",
    "logits = outputs.logits\n",
    "predictions = tf.argmax(logits, axis=1).numpy()\n",
    "\n",
    "for text, pred in zip(texts, predictions):\n",
    "    label = \"Real\" if pred == 1 else \"Fake\"\n",
    "    print(f\"[{label}] {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m pred_labels = []\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m val_dataset.batch(\u001b[32m16\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     input_ids = \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      8\u001b[39m     attention_mask = batch[\u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      9\u001b[39m     labels = batch[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mTypeError\u001b[39m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "for batch in val_dataset.batch(16):\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    labels = batch['label']\n",
    "    \n",
    "    outputs = model({'input_ids': input_ids, 'attention_mask': attention_mask})\n",
    "    preds = tf.argmax(outputs.logits, axis=1)\n",
    "    \n",
    "    true_labels.extend(labels.numpy())\n",
    "    pred_labels.extend(preds.numpy())\n",
    "\n",
    "\n",
    "print(classification_report(true_labels, pred_labels, target_names=[\"Fake\", \"Real\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.75      0.68      0.72       570\n",
      "        Real       0.90      0.93      0.91      1750\n",
      "\n",
      "    accuracy                           0.87      2320\n",
      "   macro avg       0.83      0.80      0.81      2320\n",
      "weighted avg       0.86      0.87      0.86      2320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "for batch in val_dataset.batch(16):\n",
    "    inputs, labels = batch  \n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    \n",
    "    outputs = model({'input_ids': input_ids, 'attention_mask': attention_mask}, training=False)\n",
    "    preds = tf.argmax(outputs.logits, axis=1)\n",
    "    \n",
    "    true_labels.extend(labels.numpy())\n",
    "    pred_labels.extend(preds.numpy())\n",
    "\n",
    "print(classification_report(true_labels, pred_labels, target_names=[\"Fake\", \"Real\"]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
